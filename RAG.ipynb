{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785c5b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\devan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (0.3.29)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\devan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\devan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (5.1.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\devan\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.3.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (3.12.15)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (0.6.7)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (2.0.37)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (0.3.76)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.32.5 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (0.4.29)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\devan\\anaconda3\\lib\\site-packages (from faiss-cpu->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (11.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (4.56.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (0.35.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (2.10.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (6.6.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 1)) (24.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community->-r requirements.txt (line 1)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 3)) (2025.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\devan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (0.25.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (3.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 4)) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->-r requirements.txt (line 1)) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\devan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (2.8.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers->-r requirements.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 3)) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 3)) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (1.5.1)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\devan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\devan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (1.0.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\devan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community->-r requirements.txt (line 1)) (2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (2.0.1)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.9.1\n",
      "    Uninstalling scipy-1.9.1:\n",
      "      Successfully uninstalled scipy-1.9.1\n",
      "Successfully installed scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf19e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings #to perform word embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "971940b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key= key)\n",
    "llm_model= genai.GenerativeModel('gemini-2.5-flash-lite')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4da8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_24928\\30644062.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name = 'all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445fd4db316143cdb7460b7c0623616c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\devan\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307d8a1776324484be5fc3a0295e208e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b4bc558c5a46e2915e01caaf05fb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7994b6243d4f41d0aedb125459f20ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d4133e5c054e2faff37defa1133dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9252bdb755741aea4e5439777dd2bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa8c92e57784a9cab5b7b5e579dc335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5054db2db1b64133a25e3755aefc1c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b059971941b44f3e8a6fc9ed1e210f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d18a5147fbe43468ce1ed3b3fd754ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15534491b8b045fb81985a3859d4cabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name = 'all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6b2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_file = PdfReader(r'c:\\Users\\devan\\OneDrive\\Desktop\\Great Learning All Files\\NLP\\WEEK3\\oUyZVj.pdf')\n",
    "raw_text = ''\n",
    "\n",
    "for page in loaded_file.pages:\n",
    "    text_only = page.extract_text() # this will skip the text if there is no context\n",
    "    if text_only:\n",
    "        raw_text += text_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ea31cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Study: RAG Chatbot Powered by Google \n",
      "Gemini for Smart Document Q&A \n",
      "Project Title: Intelligent Document Q&A Assistant using Retrieval-Augmented Generation \n",
      "(RAG) with Gemini \n",
      "GitHub Repository: https://github.com/mukul-mschauhan/RAG-Using-Gemini \n",
      "Live Demo: https://gemini-rag2025.streamlit.app/ \n",
      " \n",
      "Problem Statement \n",
      "Across industries such as legal, finance, healthcare, and construction, professionals are \n",
      "required to extract insights from massive document repositories—contracts, product \n",
      "manuals, policies, reports, regulations, and emails. \n",
      "Traditional keyword-based search and static FAQs fail to deliver contextual, accurate \n",
      "answers. Employees waste hours scanning PDFs and notes, leading to operational \n",
      "inefficiencies, poor decision-making, and knowledge silos. \n",
      "There’s a critical need for an intelligent assistant that can understand natural language \n",
      "questions, reason over domain-specific documents, and deliver precise responses—\n",
      "instantly. \n",
      " \n",
      "Business Objective \n",
      "To build an enterprise-grade, scalable, and cost-efficient RAG-powered AI assistant that \n",
      "enables: \n",
      " Smart retrieval and summarization of large-scale PDF/text content \n",
      " Natural language understanding of user queries \n",
      " Domain-specific knowledge augmentation via vector search \n",
      " Real-time Q&A over user-uploaded documents \n",
      "This tool is aimed at: - Legal & Compliance teams automating contract review - Analysts \n",
      "querying internal policy documents - Researchers navigating technical papers - Support \n",
      "teams handling customer manuals and SOPs \n",
      " Proposed Solution \n",
      "We built a fully functional web application that allows users to: \n",
      "1. Upload one or more PDF/text documents \n",
      "2. Extract content and embed it using state-of-the-art vectorization (Google Gemini-\n",
      "compatible) \n",
      "3. Store and retrieve relevant chunks using vector search (FAISS) \n",
      "4. Ask questions in natural language \n",
      "5. Get contextual answers generated by Google Gemini 1.5 Flash using the retrieved \n",
      "documents \n",
      " \n",
      "Architecture Overview \n",
      "1. Frontend: Streamlit web UI for uploading files and chat interface \n",
      "2. Document Processing: Text extraction using PyMuPDF and chunking logic \n",
      "3. Embeddings: Google-compatible embeddings (e.g., Gemini/Vertex-compatible or \n",
      "from SentenceTransformers) \n",
      "4. Vector Store: FAISS for similarity search on embedded text chunks \n",
      "5. LLM Integration: Google Gemini 1.5 Flash for natural language generation using \n",
      "retrieved chunks as context \n",
      "6. Prompt Engineering: Carefully crafted system and user prompts to ensure \n",
      "contextual relevance and safety \n",
      " \n",
      "Tech Stack \n",
      "Layer Tools Used \n",
      "LLM Google Gemini 1.5 Flash \n",
      "Vector DB FAISS \n",
      "Embeddings SentenceTransformers / Gemini \n",
      "Text Extraction PyMuPDF (fitz) \n",
      "Frontend Streamlit \n",
      "Backend Integration LangChain \n",
      "Deployment Streamlit Cloud \n",
      "Programming Language Python \n",
      " \n",
      "Business Impact \n",
      " Reduced document navigation time by 90% \n",
      " Enabled 24x7 AI assistant for contract, policy, and research Q&A  Democratized document access for non-technical users \n",
      " Scalable for internal or customer-facing use cases \n",
      "Example Use Cases: - Ask a policy question like: “What is the refund timeline for \n",
      "cancelled trips?” - Upload 5 contracts and ask: “Which clause discusses penalty on late \n",
      "delivery?” - Upload medical SOPs and ask: “When to escalate Stage 2 hypertension?” \n",
      " \n",
      "Conclusion & Future Roadmap \n",
      "This RAG solution bridges the gap between static document stores and intelligent, real-\n",
      "time assistance. \n",
      "Next Enhancements: - Multi-user support with user-based document history - Support for \n",
      "audio documents via speech-to-text - API endpoints for enterprise SaaS integration - In-\n",
      "built analytics dashboard \n",
      " \n",
      "For the complete implementation, visit the GitHub repo: 跚跛跜距 https://github.com/mukul-\n",
      "mschauhan/RAG-Using-Gemini \n",
      "Try the live chatbot: 뜜뜝뜡뜢뜞뜟뜠 https://gemini-rag2025.streamlit.app/ \n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d5369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 we create chunks (create chunks)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 300,chunk_overlap = 50)\n",
    "chunks = splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6626732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(chunks,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acf38abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs = {'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24677093",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'show me the steps to proceed with this project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93e6ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_24928\\714890638.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query = query)\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18717fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Demo: https://gemini-rag2025.streamlit.app/ \n",
      " \n",
      "Problem Statement \n",
      "Across industries such as legal, finance, healthcare, and construction, professionals are \n",
      "required to extract insights from massive document repositories—contracts, product \n",
      "manuals, policies, reports, regulations, and emails.\n",
      "4. Ask questions in natural language \n",
      "5. Get contextual answers generated by Google Gemini 1.5 Flash using the retrieved \n",
      "documents \n",
      " \n",
      "Architecture Overview \n",
      "1. Frontend: Streamlit web UI for uploading files and chat interface\n",
      "Case Study: RAG Chatbot Powered by Google \n",
      "Gemini for Smart Document Q&A \n",
      "Project Title: Intelligent Document Q&A Assistant using Retrieval-Augmented Generation \n",
      "(RAG) with Gemini \n",
      "GitHub Repository: https://github.com/mukul-mschauhan/RAG-Using-Gemini\n"
     ]
    }
   ],
   "source": [
    "for i in retrieved_docs:\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f51ca8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Live Demo: https://gemini-rag2025.streamlit.app/ \\n \\nProblem Statement \\nAcross industries such as legal, finance, healthcare, and construction, professionals are \\nrequired to extract insights from massive document repositories—contracts, product \\nmanuals, policies, reports, regulations, and emails. 4. Ask questions in natural language \\n5. Get contextual answers generated by Google Gemini 1.5 Flash using the retrieved \\ndocuments \\n \\nArchitecture Overview \\n1. Frontend: Streamlit web UI for uploading files and chat interface Case Study: RAG Chatbot Powered by Google \\nGemini for Smart Document Q&A \\nProject Title: Intelligent Document Q&A Assistant using Retrieval-Augmented Generation \\n(RAG) with Gemini \\nGitHub Repository: https://github.com/mukul-mschauhan/RAG-Using-Gemini'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = ' '.join([doc.page_content for doc in retrieved_docs])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a670480",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f''' you are a helpful assitant using RAG here is the context = {context}\n",
    "The query asked by user is as follows = {query}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc73ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the steps to proceed with this project:\n",
      "\n",
      "1.  **Understand the Problem:** The core problem is extracting insights from large document repositories across various industries (legal, finance, healthcare, construction). This involves dealing with diverse document types like contracts, manuals, policies, reports, regulations, and emails.\n",
      "\n",
      "2.  **Leverage RAG and Gemini:** The project utilizes Retrieval-Augmented Generation (RAG) powered by Google Gemini 1.5 Flash to address this problem. This means:\n",
      "    *   **Retrieval:** Documents will be processed and stored in a way that allows for efficient searching and retrieval of relevant information based on user queries.\n",
      "    *   **Augmentation:** The retrieved information will be used to augment the prompt given to the Gemini model.\n",
      "    *   **Generation:** Google Gemini 1.5 Flash will then generate contextual answers based on the augmented prompt.\n",
      "\n",
      "3.  **Build the Frontend (Streamlit):**\n",
      "    *   Develop a Streamlit web UI.\n",
      "    *   This UI will handle file uploads from users.\n",
      "    *   It will also provide a chat interface for users to ask questions.\n",
      "\n",
      "4.  **Implement the Core RAG Logic:**\n",
      "    *   **Document Ingestion and Processing:** This would involve taking uploaded documents, parsing their content, and potentially chunking them into smaller, manageable pieces for efficient retrieval.\n",
      "    *   **Vector Embeddings:** Create vector embeddings for the document chunks. This allows for semantic similarity searches.\n",
      "    *   **Vector Database/Index:** Store these embeddings in a vector database or index for fast retrieval.\n",
      "    *   **Query Processing:** When a user asks a question, convert it into a vector embedding.\n",
      "    *   **Retrieval:** Search the vector database for document chunks whose embeddings are most similar to the query embedding.\n",
      "    *   **Prompt Engineering:** Construct a prompt for Gemini that includes the user's original question and the retrieved document content.\n",
      "\n",
      "5.  **Integrate with Google Gemini 1.5 Flash:**\n",
      "    *   Use the Gemini API to send the constructed prompt to the model.\n",
      "    *   Receive the generated answer from Gemini.\n",
      "\n",
      "6.  **Display Answers in the UI:**\n",
      "    *   Present the contextual answers generated by Gemini back to the user through the Streamlit chat interface.\n",
      "\n",
      "7.  **Deployment (Optional but implied by the live demo):**\n",
      "    *   Deploy the Streamlit application to a platform that allows public access (like Streamlit Cloud, as suggested by the demo link).\n",
      "\n",
      "**In summary, the steps involve setting up a user interface for file uploads and chat, processing those files to create a searchable knowledge base, using a retrieval mechanism to find relevant information for a user's query, and then leveraging Google Gemini 1.5 Flash to generate an informed answer based on both the query and the retrieved content.**\n",
      "\n",
      "The provided GitHub repository (https://github.com/mukul-mschauhan/RAG-Using-Gemini) would be the primary resource for detailed implementation steps and code.\n"
     ]
    }
   ],
   "source": [
    "print(llm_model.generate_content(prompt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960641e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
